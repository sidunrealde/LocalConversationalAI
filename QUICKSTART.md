# Quick Start Guide - Phase 1 Voice Chat

## ğŸ¯ 5-Minute Overview

```
Your Code                 Your Computer
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€          â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
chat.py         â”€â”€â”€â”€â†’    Streamlit UI
                         (http://localhost:8501)
                              â†“
                         [You type message]
                              â†“
modules/ollama_client.py â”€â”€â”€â”€â†’ Ollama LLM
                             (qwen2.5:7b)
                              â†“
modules/tts.py          â”€â”€â”€â”€â†’ Piper TTS
                             (audio synthesis)
                              â†“
                         [You hear response]
```

## ğŸ“‹ Pre-Start Checklist

- [ ] Python 3.10+ installed
- [ ] Ollama downloaded (https://ollama.ai)
- [ ] RTX 3090 available (or similar GPU)
- [ ] 20+ GB free disk space
- [ ] Internet connection (for model downloads)

## âš¡ Quick Start (3 Steps)

### Step 1ï¸âƒ£: Environment Setup (5 min)

```bash
# Open PowerShell in project folder

cd f:\Projects\LLM\LocalConversationalAI

# Create virtual environment
python -m venv venv

# Activate
.\venv\Scripts\Activate.ps1

# Install all packages (grab coffee â˜•)
pip install -r requirements.txt
```

**Expected**: No errors, lots of downloading

### Step 2ï¸âƒ£: Start Ollama (3 min)

**Open 2 new PowerShell windows**

**PowerShell #1** - Start Ollama:
```bash
ollama serve
```

Expected output:
```
Listening on 127.0.0.1:11434
```

**PowerShell #2** - Download model:
```bash
ollama pull qwen2.5:7b
```

Expected output:
```
pulling manifest...
downloading model...
(waits 5-15 min)
```

### Step 3ï¸âƒ£: Run App (2 min)

**PowerShell #3** (with venv activated):
```bash
streamlit run chat.py
```

Expected output:
```
  You can now view your Streamlit app in your browser.
  Local URL: http://localhost:8501
```

**Browser**: Opens automatically to `http://localhost:8501`

**Wait for**: "âœ… Engines ready!" message (1-5 min on first run)

## âœ… First Test (Text Input)

1. **Sidebar Check**: Should show configuration options
2. **Type Message**: `"Hello, who are you?"`
3. **Click Send** (or press Enter)
4. **See Response**:
   - Text appears
   - Audio player shows below
   - Click play ğŸµ to hear response

**Congrats!** ğŸ‰ You have a working voice chat agent!

---

## ğŸ“Š What Just Happened

```
INPUT: "Hello, who are you?"
   â†“
[Streamlit App]
   â†“
[Ollama LLM] "I'm a helpful AI assistant..."
   â†“
[Piper TTS] Generates audio from text
   â†“
OUTPUT: Audio plays + text shown
```

---

## ğŸ§ª Quick Tests (15 min)

### Test 1: Change Personality

1. **Sidebar** â†’ "Agent Personality" dropdown
2. Change to: "Witty Unreal Engine expert from Bengaluru"
3. Type: `"What is machine learning?"`

**Expected**: Response sounds different (more witty/casual)

### Test 2: Change Voice

1. **Sidebar** â†’ "Voice Settings" 
2. Change voice to: "bryce"
3. Type: `"Say hello in a friendly way"`
4. Click play on audio

**Expected**: Voice sounds different (smoother, different tone)

### Test 3: See History

1. Have 3-4 exchanges
2. Refresh page (F5)
3. Look at chat

**Expected**: All previous messages still there

### Test 4: Temperature (Creativity)

1. **Sidebar** â†’ Expand "Advanced Options"
2. Set Temperature to **0.2** (low)
3. Type: `"Give me a creative game idea"`
4. Note response
5. Set Temperature to **0.95** (high)
6. Clear conversation
7. Type same question

**Expected**: Low = repetitive, High = more varied

---

## ğŸ› Troubleshooting Quick Reference

| If You See | Do This |
|------------|---------|
| "Ollama server is not running" | Go to PowerShell #1, check `ollama serve` is running |
| App stuck loading | Wait 2-5 min, it's downloading models |
| No audio | Check browser speaker icon ğŸ”Š not muted |
| Slow responses | Normal (5-13s). Check GPU: `nvidia-smi` |
| Models not downloading | Check internet, check logs: `logs/agent_*.log` |

**View Logs**:
- In app: Sidebar â†’ "Show Debug Info" â†’ "Recent Logs"
- Or in terminal: `tail logs/agent_*.log`

---

## ğŸ“ What to Explore Next

### Option A: Test Thoroughly
â†’ Follow [TESTING.md](TESTING.md) for 10 full checkpoints (25 min)

### Option B: Understand Code
â†’ Read module docstrings:
- Start: `modules/logger.py`
- Then: `modules/tts.py`
- Advanced: `modules/ollama_client.py`

### Option C: Customize
â†’ Edit `chat.py` to:
- Change default personality
- Add new voice options
- Tweak UI layout

### Option D: Next Phase
â†’ When ready for Phase 2 (Knowledge Scoping):
- See [SETUP.md](SETUP.md) Phase 2 section
- Implement document upload + RAG

---

## ğŸ“± 3-Window Layout (Recommended)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                              â”‚
â”‚ PowerShell #1           PowerShell #2        â”‚
â”‚ (Ollama serve)          (Downloads models)   â”‚
â”‚                                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                              â”‚
â”‚         PowerShell #3 + Browser              â”‚
â”‚      (Streamlit app + chat UI)               â”‚
â”‚                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸš€ Expected Timeline

```
T+0:00   Install packages
         â†“
T+5:00   Start Ollama
         â†“
T+8:00   Run app
         â†“
T+13:00  "âœ… Engines ready!"
         â†“
T+13:30  First message sent
         â†“
T+20:00  Testing features
         â†“
T+25:00  Ready for Phase 2!
```

**Key Milestones**:
- âœ… 5 min: Dependencies installed
- âœ… 8 min: Model downloaded
- âœ… 13 min: App ready
- âœ… 25 min: Full test suite passing

---

## ğŸ“ Need Help?

1. **Check logs**: `logs/agent_*.log`
2. **Read docs**:
   - [README.md](README.md) - Overview
   - [SETUP.md](SETUP.md) - Detailed setup
   - [TESTING.md](TESTING.md) - Full test checklist
3. **Check GPU**: `nvidia-smi` (should show Ollama processes)
4. **Restart services**: Ctrl+C in terminals and restart

---

## ğŸ¯ Success Checklist

- [ ] Python 3.10+ running
- [ ] pip packages installed (no errors)
- [ ] Ollama server responding (`ollama list` works)
- [ ] Model downloaded (qwen2.5:7b shows in `ollama list`)
- [ ] Streamlit opens in browser
- [ ] "âœ… Engines ready!" appears
- [ ] Can type and get text response
- [ ] Audio plays after response
- [ ] Personality switching works
- [ ] Voice switching works

**All checked?** â†’ Phase 1 âœ… DONE! Go to [TESTING.md](TESTING.md) for full validation.

---

**Now open `http://localhost:8501` and start chatting! ğŸ‰**
